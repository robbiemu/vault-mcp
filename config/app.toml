# Vault MCP Server Configuration

[paths]
# Update this path to point to your Obsidian vault
vault_dir = "/Users/Shared/Public/Github/vault-mcp/tests/assets/vault"
# Directory to store the ChromaDB vector database.
# It is strongly recommended to use an absolute path.
database_dir = "./chroma_db"

[prefix_filter]
# Optional: Only index files with these prefixes (empty list = include all .md files)
allowed_prefixes = [
  # Only index files starting with these prefixes
  "AI",
  "Machine"
  # "Resource Balance Game",
  # "Decentralized Multiplayer Game System"
  # Add more prefixes as needed:
  # "Project Documentation",
  # "Meeting Notes", 
  # "Research"
]

[indexing]
chunk_size = 150  # Small chunks to test section expansion
chunk_overlap = 20
quality_threshold = 0.75
# Enable to filter out low-quality chunks based on the quality_threshold.
# Recommended for "noisy" source data, but off by default to ensure all
# content from curated vaults is indexed.
enable_quality_filter = false

[watcher]
enabled = true
debounce_seconds = 2

[server]
host = "127.0.0.1"
port = 8000
default_query_limit = 5

# ===== MODEL CONFIGURATION =====

[embedding_model]
# Choose embedding provider: "sentence_transformers", "mlx_embeddings", "openai_endpoint"
provider = "openai_endpoint"
model_name = "yoeven/multilingual-e5:large-it-Q5_K_M"
endpoint_url = "http://localhost:11434/v1"
api_key = "ollama"
wrapper_class = "plugins.e5_instruct_wrapper.E5InstructWrapper"

# For MLX on Apple Silicon (experimental):
# provider = "mlx_embeddings"
# model_name = "mlx-community/mxbai-embed-large-v1"

# For OpenAI-compatible API (like Ollama):
# provider = "openai_endpoint"
# model_name = "nomic-embed-text"
# endpoint_url = "http://localhost:11434/v1"
# api_key = "ollama"

[generation_model]
# LiteLLM model identifier. Examples:
# - Local Ollama: "ollama/llama3", "ollama/mistral", "ollama/codellama"
# - OpenAI: "gpt-4o-mini", "gpt-4o" (requires OPENAI_API_KEY)
# - Anthropic: "claude-3-haiku-20240307" (requires ANTHROPIC_API_KEY)
# - Azure: "azure/gpt-4" (requires AZURE_API_KEY and AZURE_API_BASE)
model_name = "ollama/llama3"

# Parameters passed to litellm.completion()
[generation_model.parameters]
temperature = 0.5
max_tokens = 1024

[retrieval]
# Choose retrieval mode: "static" or "agentic"
# - static: Return full sections containing matched chunks (no LLM required)
# - agentic: Use LLM to rewrite chunks for relevance (requires generation_model)
mode = "static"

# ===== EXAMPLE CONFIGURATIONS =====
# Copy any of these to your config's app.toml to override the defaults:

# OpenAI GPT-4o Mini:
# [generation_model]
# model_name = "gpt-4o-mini"
# [generation_model.parameters]
# temperature = 0.3
# max_tokens = 2000

# Anthropic Claude:
# [generation_model]
# model_name = "claude-3-haiku-20240307"
# [generation_model.parameters]
# temperature = 0.4
# max_tokens = 1500

# Local Ollama with different model:
# [generation_model]
# model_name = "ollama/mistral"
# [generation_model.parameters]
# temperature = 0.6
# max_tokens = 1500
