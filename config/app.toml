# Vault MCP Server Configuration

[paths]
# Update this path to point to your Obsidian vault
vault_dir = "/absolute/path/to/obsidian-vault"

[prefix_filter]
# Optional: Only index files with these prefixes (empty list = include all .md files)
allowed_prefixes = [
  "Resource Balance Game",
  "Decentralized Multiplayer Game System"
  # Add more prefixes as needed:
  # "Project Documentation",
  # "Meeting Notes", 
  # "Research"
]

[indexing]
chunk_size = 512
chunk_overlap = 64
quality_threshold = 0.75

[watcher]
enabled = true
debounce_seconds = 2

[server]
host = "127.0.0.1"
port = 8000

# ===== MODEL CONFIGURATION =====

[embedding_model]
# Choose embedding provider: "sentence_transformers", "mlx_embeddings", "openai_endpoint"
provider = "sentence_transformers"
model_name = "all-MiniLM-L6-v2"

# For MLX on Apple Silicon (experimental):
# provider = "mlx_embeddings"
# model_name = "mlx-community/mxbai-embed-large-v1"

# For OpenAI-compatible API (like Ollama):
# provider = "openai_endpoint"
# model_name = "nomic-embed-text"
# endpoint_url = "http://localhost:11434/v1"
# api_key = "ollama"

[generation_model]
# LiteLLM model identifier. Examples:
# - Local Ollama: "ollama/llama3", "ollama/mistral", "ollama/codellama"
# - OpenAI: "gpt-4o-mini", "gpt-4o" (requires OPENAI_API_KEY)
# - Anthropic: "claude-3-haiku-20240307" (requires ANTHROPIC_API_KEY)
# - Azure: "azure/gpt-4" (requires AZURE_API_KEY and AZURE_API_BASE)
model_name = "ollama/llama3"

# Parameters passed to litellm.completion()
[generation_model.parameters]
temperature = 0.5
max_tokens = 1024

# ===== EXAMPLE CONFIGURATIONS =====
# Copy any of these to your config's app.toml to override the defaults:

# OpenAI GPT-4o Mini:
# [generation_model]
# model_name = "gpt-4o-mini"
# [generation_model.parameters]
# temperature = 0.3
# max_tokens = 2000

# Anthropic Claude:
# [generation_model]
# model_name = "claude-3-haiku-20240307"
# [generation_model.parameters]
# temperature = 0.4
# max_tokens = 1500

# Local Ollama with different model:
# [generation_model]
# model_name = "ollama/mistral"
# [generation_model.parameters]
# temperature = 0.6
# max_tokens = 1500
